[БЕЗ_ЗВУКА] Мы начинаем урок, посвященный визуализации данных. И давайте сначала обсудим, зачем вообще данные нужно визуализировать. Когда мы на прошлой неделе говорили про понижение размерности, мы обсуждали вот такой простой пример. Представьте, что данные выглядят вот так, видно, что закономерности явно нелинейные, и вы хотите понизить размерность до одного признака. Если вы будете пытаться спроецировать эти данные, эту выборку на прямую, то есть будете пытаться применить линейный подход к понижению размерности, то у вас ничего не получится. Какую прямую вы бы ни выбирали, вы будете терять довольно много информации. Поэтому, чтобы информацию сохранить, нужно проецировать выборку на красную кривую, которая явно нелинейная, и ее нужно подбирать, используя нелинейные методы понижения размерности. Вообще в машинном обучении мы имеем дело с высокоразмерными выборками, то есть с выборками, в которых очень много признаков. Но при этом мы хотим как-то смотреть на эти данные, понимать, как они устроены, какие там взаимосвязи, какие признаки важны, а какие нет, как соотносятся классы между собой. Чтобы это делать, можно нарисовать, например, вот такую картинку, то есть для всех пар признаков изобразить проекцию выборки на эту пару признаков, то есть пытаться понять, как соотносятся именно эти два признака между собой, а также для каждого одного признака нарисовать гистограмму распределения. Конечно, по этим картинкам можно что-то понять. Можно заметить, что какие-то пары признаков лучше разделяют классы или что какой-то один признак очень неплохо их разделяет, но при этом мы не видим картины в целом, мы не видим, как взаимодействуют признаки между собой в совокупности. Хотелось бы отобразить всю выборку в двумерное или трехмерное пространство, так чтобы по вот такой, например, картинке сразу были видны все закономерности в данных, вся структура этих данных. Например, что какие-то классы сильно перемешаны между собой, какие-то классы выделяются и их можно хорошо отделить от всего остального и так далее. Собственно, именно так мы приходим к задаче визуализации данных. Это частный случай нелинейного понижения размерности, когда размерность пространства, в которое мы пытаемся спроецировать нашу выборку, это 2 или 3 — двумерное пространство, плоскость, или трехмерное пространство, то есть просто пространство. 2 или 3, потому что мы умеем отображать данные только в пространствах размерности 2 или 3. В пространствах большей размерности мы мыслить не умеем. И при этом мы хотим спроецировать данные так, чтобы сохранить всю их структуру, чтобы после проецирования сохранились все закономерности, и мы могли их наблюдать по этой картинке. Один из классических примеров, на которых рассматривают задачу визуализации данных, это набор данных MNIST. Этот набор данных состоит из некоторого количества изображений и цифр, написанных от руки. Понятно, что все люди пишут цифры по-разному: кто-то их наклоняет, кто-то не наклоняет, кто-то делает засечки, кто-то не делает и так далее. Эти цифры очень разные. И при этом нужно их разделить на 10 классов, то есть определить по изображению, что за цифра на нем нарисована. В этом датасете каждая картинка имеет размер 28 x 28 пикселей, то есть всего каждый объект имеет 784 признака, но при этом понятно, что внутренняя размерность данных гораздо ниже. Мы могли бы использовать гораздо меньше признаков, чтобы характеризовать каждую рукописную цифру. Это можно понять, например, на простом примере. Если мы будем генерировать случайные картинки размера 28 x 28 пикселей, то будем получать что-то вот такое. На этих картинках нет ничего общего с изображениями цифр, это просто какие-то случайные изображения такого же размера. При этом вероятность того, что мы при такой случайной генерации получим именно изображение цифры или что-то похожее на нее, крайне мала. Хотя бы это дает понять, что внутренняя размерность сильно ниже. Если мы воспользуемся одним из методов визуализации, о которых будем говорить в этом уроке, то получим вот такое изображение. Всего лишь двух признаков хватит, чтобы изобразить наши цифры так, что все классы будут идеально разделимы, и при этом будут видны некоторые интересные закономерности — какие-то классы совсем не похожи на остальные, а какие-то находятся близко, а значит, что начертание у них довольно похожее друг на друга. Итак, можно поговорить про пример еще одной задачи. Эта задача используется в одном из конкурсов на сайте Kaggle. В ней требовалось взять описание товара, его характеристики, и по этому описанию понять, к какой из девяти категорий этот товар относится, то есть задача была о классовой классификации. При этом признаков было 93, то есть описание довольно высокоразмерное. Если изобразить эту выборку опять же с помощью одного из методов визуализации данных, то можно получить вот такое изображение, на котором видно, что некоторые классы хорошо выделяются в отдельные кластеры, а какие-то классы очень сильно перемешиваются между собой. И при этом эта визуализация очень качественная, в том смысле, что даже если применять сложные методы, вроде градиентного бустинга или случайных лесов для разделения классов в исходном пространстве, пространстве размерности 93, то там эти свойства будут сохраняться. Те классы, которые здесь хорошо отделимы, будут отделимы и там, а те классы, которые здесь сильно перемешаны между собой, даже в пространстве высокой размерности будет очень сложно разделить, ошибка для них будет довольно большой. Мы обсудили задачу визуализации данных. Это задача, в которой требуется отобразить выборку пространства размерности 2 или 3, и это задача, которая нужна для того, чтобы как-то посмотреть на данные, понять, какую структуру они имеют внутри себя. В следующем видео мы поговорим о том, какими методами можно данные визуализировать, что можно использовать для решения этой задачи.

[БЕЗ_ЗВУКА] Привет! В этом видео мы продолжим говорить про задачу визуализации данных и также на примерах рассмотрим все тем методы, которые вы уже успели изучить. Работать мы будем с датасетом digits. Это стандартный датасет, который представляет из себя написанные от руки цифры. Вот давайте его загрузим. Делаем это с помощью модуля datasets из библиотеки sklearn и теперь посмотрим, что же он из себя представляет. Посмотрим на описание. Мы видим, что набор представляет из себя изображения цифр, всего их 5620. Каждая цифра представляется в виде вектора из 64 цифр, каждая из которых характеризует яркость пикселя в данной точке. Соответственно, изображение представляет из себя прямоугольник 8 x 8. Мы видим, что пропущенных значений у нас, конечно же нет, и также у нас есть ссылка на uci репозиторий, в котором вы можете ознакомиться с полным описанием датасета и скачать его. Итак, теперь давайте посмотрим, как же выглядят данные, и как представлены целевые метки. Вот мы видим, что... Смотрим на первый объект, у него целевая метка 0 — означает, что это рукописная цифра 0, и, собственно, мы видим набор его признаков. Вектор длиной 64, изменяющийся в диапазоне от 0 до 16. Теперь давайте попробуем его отрисовать, посмотреть, как же эта картинка выглядит. Для того чтобы отрисовывать изображения, Python предоставляет нам функцию imshow. Эта функция библиотеки Matplotlib. Вот давайте попробуем сделать следующее. Передадим этой функции в качестве аргумента описание нашего объекта и попробуем его нарисовать. Потренируемся на самом первом объекте. Видим, что почему-то ничего не получилось. Давайте смотреть, почему. Написано, что размерность нашего объекта, размерность данных, не подходит для того, чтобы строить рисунок. Давайте посмотрим, какая же у нас размерность. Видим, что размерность (64, 0), то есть фактически мы передаем в данную функцию не матрицу, а вектор, поэтому непонятно, как же этот вектор отобразить. Что мы с вами можем сделать? Мы можем вспомнить, что ndarray предоставляет нам функцию reshape, которая позволяет изменить размерность нашего вектора. Вот давайте, вместо того чтобы работать со строчкой длиной 64, будем работать с матрицей длиной 8 x 8. Для этого вызовем метод reshape с параметрами 8 и 8. Давайте посмотрим. Видим, что данные не изменились, но теперь это уже не вектор, а матрица, поэтому кажется, что в этом виде можно попробовать нарисовать. На всякий случай вызываем метод shape, видим, что размерность правильная, и теперь давайте рисовать. Вот, видим, что теперь все получилось. Перед нами изображение рукописной цифры, и мы знаем, что это 0. На самом деле, создатели датасета уже позаботились о том, чтобы нам было удобно отображать цифры и не нужно было каждый раз делать reshape. Давайте посмотрим на все те атрибуты, которые предоставляет наш датасет. Видим, что помимо данных целевой переменной, имен классов и описаний у нас есть еще ключ images. Давайте посмотрим, что это такое. Видим, что это фактически то же самое описание объекта, но только уже не в виде вектора, который очень удобен для решения задач классификации или кластеризации, а в виде матрицы, которая прекрасно подходит для того, чтобы объект рисовать. Вот давайте его попробуем отрисовать, используя images вместо data. И видим, что получаем ту же самую картинку, еще и удобно — не нужно делать reshape. Теперь давайте посмотрим, как мы можем влиять на то, какое изображение получается. Возможно, в таком виде не очень видно, что это 0, изображение довольно яркое, разноцветное. На это можно легко повлиять. Во-первых, мы можем влиять на то, в какой цветовой карте, в каком colormap отображается наше изображение. Мы можем выбрать любое. Например, выбрать colormap hot или выбрать colormap gray — это черно-белые тона. Также мы можем влиять на так называемую interpolation, или параметр, который отвечает за то, как будут выглядеть границы наших пикселей на изображении. Давайте рассмотрим четыре разных варианта. И видим, что каждый может выбрать себе, что ему больше нравится. Вот мне больше всего нравится черно-белое изображение, поэтому давайте в этих же цветах отрисуем и другие цифры. Посмотрим, как они выглядят и легко ли их узнать глазами. Итак, воспользуемся subplot и отрисуем первые 10 объектов из нашего датасета. Будем надеяться, что это разные цифры. Так, видим, что мы получили изображения. Действительно, перед нами изображения цифр от 0 до 9, и, в принципе, невооруженным взглядом их узнать можно. Соответственно, это дает нам следующее предположение. Если мы с вами цифры можем довольно неплохо узнать, мы видим, что их начертания отличаются, то, наверное, можно решать задачу классификации более-менее успешно. Наверное, она решается неплохо. Из предыдущих лекций вам известно, что такое представление цифр, а именно представление в виде 64 признаков, является несколько избыточным. Для того чтобы успешно отличать эти цифры друг от друга, нам достаточно гораздо меньшей размерности данных. Давайте попробуем сделать следующее. Сначала уменьшим размерность данных теми методами, которые вы изучали, и посмотрим, как же это сказывается на качестве классификации. Для начала сделаем следующее. Давайте отрежем небольшую подвыборку из нашей выборки данных и будем работать с ними. Это нужно только для того, чтобы все наши команды, все то, что мы запускаем, работало несколько быстрее. Вы, конечно же, можете тренироваться на всем наборе данных. Вот я возьму первую тысячу. И теперь давайте посмотрим, что у меня получилась сбалансированная выборка, то есть что все объекты встречаются приблизительно в одинаковой пропорции, объекты всех классов. Сделаем это с помощью Counter. Видим, что действительно, каждый объект встречается приблизительно 100 раз — то, что нужно. И чтобы это было более наглядно, можем нарисовать гистограмму, это несложно, и видим, что да, в принципе, все объекты встречаются одинаковое количество раз. Теперь давайте построим некоторый классификатор. Так как мы хотим смотреть на то, как наши объекты отображаются на плоскости, нам подойдет метрический классификатор. Он работает на основе близости объектов, поэтому это то, что нужно. Выбираем классификатор K ближайших соседей. И давайте сделаем следующее. Обучим его на всей выборе, на всей той тысяче цифр, которые мы выбрали, и посмотрим, какое получается качество. Фактически это нам скажет о том, насколько наши объекты легко классифицировать по их изображению. Итак, мы вывели довольно подробную информацию о качестве классификации. Сделали это с помощью метода classification_report, он предоставляет нам подробную сводку о том, какую точность, полноту, f1 мы получили для каждого класса, вот можем посмотреть, что классов здесь ровно десять, а также дает нам усредненные оценки. Видим, что в среднем мы правильно классифицируем практически всю выборку. В данном случае мы видим, что классификация производится на основании пяти ближайших соседей, поэтому что мы можем сказать? Мы можем сказать, что большинство объектов находится в окружении объектов из своего класса. В общем-то, это хорошо. Это говорит о том, что наши изображения действительно похожи друг на друга, что начертания похожи, и на основе изображений мы можем решать эту задачу. Теперь давайте сделаем следующее. Если мы перейдем в пространство меньшей размерности, то есть перейдем в пространство 2, нам снова будет интересно, остается ли это условие в силе, то есть в новом пространстве наши цифры также находятся близко друг к другу или уже нет, или уже в ближайшем окружении начнут встречаться объекты чужих классов, и тогда наше качество классификации уменьшится. Вот давайте на это посмотрим. Начнем с линейных методов понижения размерности. Первый метод, который мы рассмотрим — это случайные проекции. Для начала нам нужно построить наше преобразование, делаем это с помощью модуля random_projection. Строим объект SparseRandomProjection и передаем ему следующие аргументы. Первый аргумент — это количество новых компонент, которое нас интересует. Мы с вами хотим строить изображения наших объектов, поэтому нам удобно работать с двумя компонентами. И зададим random_state. Итак, после того, как мы наш объект построили, нужно это отображение обучить на исходных данных и применить, то есть получить данные в новой размерности. Вот давайте это сделаем с помощью метода fit_predict. Итак, готово наше новое представление данных, теперь давайте его изобразим на плоскости. В данном случае мы видим набор точек, где разными цветами отображены объекты разных классов, то есть разными цветами у нас отображены разные цифры. Видим, что, в общем-то, они довольно сильно перемешаны, и так вот на глаз не очевидно, в какой области нужно ожидать встретить какую цифру. На глаз оценивать такие вещи плохо, давайте попробуем обучить классификатор. Снова обучим ту же самую модель, KNN, и сравним, насколько изменилось качество. Посмотрим, для скольких объектов в ближайшем окружении вновь находятся объекты того же самого класса. Для этого оцениваем качество на обучающей выборке и что мы видим? Наше качество в среднем упало до 50 %, то есть мы видим, что для половины объектов ближайшие пять соседей уже могут быть числами из разных классов. Давайте посмотрим на следующий метод. Это метод PCA, или метод главных компонент. Он уже находится в модуле decomposition, RandomizedPCA, и давайте снова построим преобразование, в качестве результата получим представление объектов в двумерном пространстве. Снова применяем метод fit_predict, вернее fit_transform, и смотрим на наше изображение. Видим, что на глаз изображение кажется более качественным. У нас явно появились области разных цветов, то есть мы видим, что для некоторых классов получается отделить их от других, получается сделать так, чтобы объекты этих классов были рядом друг с другом. Теперь давайте оценим это с помощью классификатора. Итак, считаем качество, и да, видим, что в среднем качество уже лучше, чем для предыдущего случая. Наша аккуратность — 0,7. То есть 70 % объектов мы все еще можем классифицировать. Представьте себе, как здорово. Мы уменьшили размерность в 32 раза, вместо 64 признаков у нас теперь всего 2, и тем не менее мы получаем довольно высокое качество классификации. Давайте двигаться дальше. Перейдем к нелинейным методам. Следующий метод, который мы рассмотрим — это multidimensional scaling, или многомерное шкалирование. Он находится в модуле manifold. И давайте построим следующий объект. Будем уменьшать размерность до двух компонент и задачу оптимизации будем решать не более чем за 100 итераций. Строим обновленные данные, делаем это с помощью метода fit_transform, и теперь давайте отобразим изображение. Кажется, что нелинейные методы должны работать даже немножечко лучше. Вот давайте посмотрим. Да, видим, что вновь мы можем отличить группы объектов, и давайте получим оценку на основе классификации. Обучаем модель и видим, что качество примерно такое же, но немножечко больше. Тоже метод довольно неплохо работает. Последний метод, который мы рассмотрим — это метод t-SNE. Его реализация также находится в модуле manifold, поэтому давайте создадим наш объект-преобразователь. Снова оставим две компоненты, инициализировать преобразование будем с помощью PCA, другой вариант — делать это с помощью случайной инициализации, или random, и давайте получим преобразование данных с помощью метода fit_transform. Так, это занимает какое-то время, потому что опять же мы решаем задачу оптимизации. А теперь давайте строить изображение. Видим, что это, наверное, самая лучшая картинка из всех, которые мы получили. Здесь очень легко убедиться в том, что объекты разных цветов, или объекты разных классов, находятся довольно далеко друг от друга. Кажется, что если мы будем решать задачу классификации в таких признаках, то мы должны получить довольно высокое качество, может быть, даже сравнимое с изначальным, с данными в изначальном пространстве. Вот давайте это проверим. Снова обучим классификатор ближайших соседей и видим, что да, действительно, наше качество в среднем равняется 0,99 по метрике accuracy, то есть практически нигде мы с вами не ошибаемся. Тоже довольно неплохой метод уменьшения размерности. А на этом мы заканчиваем. На этом уроке мы научились на практике применять такие методы уменьшения размерности, как t-SNE, multidimensional scaling, PCA, а также метод случайных проекций. На этом мы заканчиваем модуль по визуализации данных, а на следующей неделе вы познакомитесь с такими интересными задачами, как topic modeling, или тематическое моделирование.